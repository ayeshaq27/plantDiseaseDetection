{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b872aca8",
   "metadata": {},
   "source": [
    "03 — Baseline CNN (Training a Simple Convolutional Model From Scratch)\n",
    "\n",
    "This notebook builds a simple Convolutional Neural Network to classify plant diseases from the PlantVillage dataset.\n",
    "The purpose of this model is not to achieve high accuracy — instead, it helps understand:\n",
    "  - how images move through a CNN\n",
    "  - how loss and backpropagation work\n",
    "  - how training + validation loops work\n",
    "  - how PyTorch updates model weights\n",
    "\n",
    "This foundation prepares us for more advanced models like ResNet, which we will use in Notebook 04."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f10f448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root added to Python path:  c:\\workspace\\plantDiseaseDetection\n",
      "Using device:  cpu\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(project_root)\n",
    "print(\"Project root added to Python path: \", project_root)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dataset import PlantVillageDataset\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf160007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1358, 340)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dir = \"../data/PlantVillage/train\"\n",
    "val_dir = \"../data/PlantVillage/val\"\n",
    "\n",
    "classes = sorted(os.listdir(train_dir))\n",
    "train_transforms = transforms.Compose([\n",
    "  transforms.Resize((224,224)), #size that ResNet needs\n",
    "  transforms.ToTensor(), #converts image to tensor\n",
    "  # images are made of pixels 0-255, converting to tensor turns it into values 0-1, floats\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "  transforms.Resize((224,224)),\n",
    "  transforms.ToTensor(),\n",
    "])\n",
    "train_dataset = PlantVillageDataset(train_dir, transform=train_transforms)\n",
    "val_dataset = PlantVillageDataset(val_dir, transform=val_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "len(train_loader), len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1173ef76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaselineCNN(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=50176, out_features=128, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=128, out_features=38, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Simple neural network model.\n",
    "  - sets baseline performance (starting accuracy to beat)\n",
    "  - confirms pipeline (data -> model -> loss -> training) is working\n",
    "\n",
    "Two parts:\n",
    "1. Feature extractor (conv.)\n",
    "  - learns patterns from the image\n",
    "2. Classifier\n",
    "  - takes the learned patterns and decides 'which class is this?'  \n",
    "  \n",
    "\"\"\"\n",
    "\n",
    "class BaselineCNN(nn.Module): #nn.Module is like the house blueprint and BaselineCNN is custom house built from it\n",
    "  def __init__(self, num_classes):\n",
    "    super(BaselineCNN, self).__init__()\n",
    "\n",
    "    #Feature extractor\n",
    "    self.features = nn.Sequential( #container that stacks layers in order (pytorch runs them in order)\n",
    "      #Chose 3 layers only for this model to keep it simple\n",
    "      \n",
    "      #First conv layer: image -> low-level features\n",
    "      #2D Convolition, extracts features like edges and textures, uses 3X3 filter, transforming the 3 initial color channels RGB into 16 different feature maps\n",
    "      nn.Conv2d(3,16,kernel_size=3, padding=1), \n",
    "      #Rectified Linear Unit -> activation function that sets neg vals in feature map to 0\n",
    "      nn.ReLU(),\n",
    "      #Max pooling layer that reduces HW of feature maps by keeping max val within 2x2 window\n",
    "      nn.MaxPool2d(2),\n",
    "\n",
    "      #Second conv layer: deeper patterns\n",
    "      nn.Conv2d(16, 32, kernel_size=3,padding=1),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(2),\n",
    "\n",
    "      #Third Conv layer\n",
    "      nn.Conv2d(32,64, kernel_size=3,padding=1),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(2)\n",
    "    )\n",
    "\n",
    "    #classifier -> takes learned features and decides class\n",
    "    self.classifier = nn.Sequential(\n",
    "      nn.Flatten(),  #from last conv layer, output is 64 feature maps where each is 28x28 (3d tensor) which is 50176 features (too many), turn feature maps into a long vector\n",
    "      nn.Linear(64 * 28 * 28, 128), #given 50176 features, output 128 \n",
    "      nn.ReLU(),\n",
    "      nn.Linear(128, num_classes) #final prediciton, given 128 pieces of evidence, how much does each class match\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    #pass image through feature extractor\n",
    "    x = self.features(x)\n",
    "\n",
    "    #pass extracted features through classifier\n",
    "    x = self.classifier(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "num_classes = len(classes)\n",
    "model = BaselineCNN(num_classes).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41fdfd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() #compares predicitions vs labels : measures how wrong they are\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) #updates model to improve next time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9501296",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training loop\n",
    "\n",
    "def train_one_epoch(model,loader,optimizer, criterion):\n",
    "  model.train() #training mode\n",
    "  running_loss = 0\n",
    "  correct = 0\n",
    "  total = 0\n",
    "\n",
    "  for images, labels in loader:\n",
    "    images = images.to(device)  #SENDS SMALL BATCH AT A TIME\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    #1. forward pass: model makes predicitons\n",
    "    outputs = model(images)\n",
    "\n",
    "    #2. compute loss\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    #3. reset gradients (clear old calc)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    #4. backward pass: calculate gradients (figure out how much each weight contributed to the error)\n",
    "    loss.backward()\n",
    "\n",
    "    #5. update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    running_loss += loss.item()\n",
    "\n",
    "    # calculate training accuracy\n",
    "    _, preds = torch.max(outputs, 1) #take highest predicted score for each image\n",
    "    correct += (preds == labels).sum().item() #compare preds to true labels\n",
    "    total += labels.size(0)\n",
    "  #return loss and accuracy for the whole epoch\n",
    "  return running_loss / len(loader), correct / total \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7631458d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, loader, criterion):\n",
    "  model.eval() # evaluation mode\n",
    "  running_loss = 0\n",
    "  correct = 0\n",
    "  total = 0\n",
    "\n",
    "  with torch.no_grad(): #no gradients here\n",
    "    for images, labels in loader:\n",
    "      images = images.to(device)\n",
    "      labels = labels.to(device)\n",
    "\n",
    "      outputs = model(images)\n",
    "      loss = criterion(outputs, labels)\n",
    "\n",
    "      running_loss += loss.item()\n",
    "\n",
    "      _,preds = torch.max(outputs, 1)\n",
    "      correct += (preds == labels).sum().item()\n",
    "      total += labels.size(0)\n",
    "\n",
    "  return running_loss / len(loader), correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34133014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Train Loss: 0.9530, Train Acc: 0.7210\n",
      "Val Loss: 0.4919, Val Acc: 0.8467\n",
      "--------------------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.3412, Train Acc: 0.8923\n",
      "Val Loss: 0.3769, Val Acc: 0.8814\n",
      "--------------------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.1970, Train Acc: 0.9370\n",
      "Val Loss: 0.3157, Val Acc: 0.9052\n",
      "--------------------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.1327, Train Acc: 0.9569\n",
      "Val Loss: 0.3067, Val Acc: 0.9058\n",
      "--------------------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.0964, Train Acc: 0.9677\n",
      "Val Loss: 0.3021, Val Acc: 0.9147\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "  val_loss, val_acc = validate(model, val_loader, criterion)\n",
    "\n",
    "  print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "  print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "  print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "  print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6beae7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"baseline_cnn.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (plant-env)",
   "language": "python",
   "name": "plant-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
