{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92a92579",
   "metadata": {},
   "source": [
    "02 — Data Pipeline (Dataset, Transforms, DataLoader)\n",
    "\n",
    "This notebook builds the full data input pipeline for the PlantVillage classification project. Before training any deep learning model, we need a reliable way to:\n",
    " - load images from disk\n",
    " - apply preprocessing and transformations\n",
    " - convert images into tensors\n",
    " - create batches for efficient training\n",
    " - pair images with the correct class labels\n",
    "\n",
    "In this notebook, we define a custom PyTorch Dataset class, set up training and validation transforms, and build DataLoader objects to feed data into our models. We also visualize sample batches to ensure that images, labels, and shapes are all correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661c2d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "# imports for data handling\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#loads images and applies transforms\n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8705cde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"../data/PlantVillage/train\"\n",
    "val_dir = \"../data/PlantVillage/val\"\n",
    "\n",
    "classes = sorted(os.listdir(train_dir))\n",
    "print(\"Number of classes: \", len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef26b9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define transforms \n",
    "#training images = teaches models\n",
    "#validation images = tests model if learning properly\n",
    "train_transforms = transforms.Compose([\n",
    "  transforms.Resize((224,224)), #size that ResNet needs\n",
    "  transforms.ToTensor(), #converts image to tensor\n",
    "  # images are made of pixels 0-255, converting to tensor turns it into values 0-1, floats\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "  transforms.Resize((224,224)),\n",
    "  transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32583f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Custom dataset class that tells PyTorch:\n",
    "  - how to find images\n",
    "  - how to read them\n",
    "  - how to apply transforms\n",
    "  - how to return (image, label)\n",
    "\"\"\"\n",
    "\n",
    "class PlantVillageDataset(Dataset):\n",
    "  def __init__(self, root_dir, classes, transform=None):\n",
    "    self.root_dir = root_dir\n",
    "    self.transform = transform\n",
    "    self.classes = classes\n",
    "\n",
    "    self.image_paths = []\n",
    "    self.labels = []\n",
    "\n",
    "    for idx, cls in enumerate(classes):\n",
    "      class_path = os.path.join(root_dir, cls)\n",
    "      img_files = os.listdir(class_path)\n",
    "\n",
    "      for img_name in img_files:\n",
    "        self.image_paths.append(os.path.join(class_path, img_name))\n",
    "        self.labels.append(idx)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.image_paths)\n",
    "  \n",
    "  def __getitem__(self,index):\n",
    "\n",
    "    #Step 1 -> load image\n",
    "    img_path = self.image_paths[index]\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    #Step 2 -> apply transform\n",
    "    if self.transform:\n",
    "      img = self.transform(img)\n",
    "    \n",
    "    #Step 3 -> return (img, label)\n",
    "    label = self.labels[index]\n",
    "    return (img, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d733002",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PlantVillageDataset(train_dir, classes, transform=train_transforms)\n",
    "val_dataset = PlantVillageDataset(val_dir, classes, transform=val_transforms)\n",
    "\n",
    "print(\"Train samples:\", len(train_dataset))\n",
    "print(\"Val samples:\", len(val_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9ee32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaders handle batching, shuffling, and efficient reading.\n",
    "#shuffle=True -> prevents models from memorizing order\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1669918",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "print(\"Batch image tensor shape:\", images.shape)\n",
    "print(\"Batch labels:\", labels[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef5e833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(images, labels, classes, n=8):\n",
    "  plt.figure(figsize=(18, 6))\n",
    "    \n",
    "  for i in range(n):\n",
    "    img = images[i].permute(1, 2, 0)  # Channel,Height,Width → HWC (for plotting)\n",
    "        \n",
    "    plt.subplot(1, n, i + 1)\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "        \n",
    "     # wrap long class names\n",
    "    label = classes[labels[i]]\n",
    "    plt.title(label, fontsize=8, wrap=True)\n",
    "    \n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "show_batch(images, labels, classes)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (plant-env)",
   "language": "python",
   "name": "plant-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
